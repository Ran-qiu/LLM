# LLM 统一管理平台 - 开发文档

## 1. 项目概述

### 1.1 项目简介

本项目是一个本地部署的LLM统一管理平台，支持主流大模型厂商的API调用，提供统一的对话界面、历史记录管理和用户认证功能。

### 1.2 核心功能

- **多厂商LLM支持**：统一接口调用OpenAI、Claude、Gemini等主流大模型
- **Ollama集成**：支持本地部署的开源模型（Llama、Mistral等）
- **LangChain集成**：利用LangChain强大的LLM编排能力，支持高级对话场景
- **自定义模型支持**：支持配置任何兼容OpenAI API格式的自定义模型服务
- **n8n工作流自动化**：集成n8n实现LLM工作流编排、定时任务、Webhook触发等自动化场景
- **用户认证系统**：支持用户注册、登录、权限管理
- **对话管理**：创建、编辑、删除对话会话
- **历史记录**：完整的对话历史存储和检索
- **API密钥管理**：安全存储和管理各厂商的API密钥，支持自定义URL和参数
- **本地化部署**：数据完全本地存储，保护隐私

### 1.3 支持的LLM厂商

- **OpenAI**：GPT-4, GPT-3.5等系列模型
- **Anthropic**：Claude 3.5 Sonnet, Claude 3 Opus等
- **Google**：Gemini Pro, Gemini Ultra
- **阿里云**：通义千问系列
- **百度**：文心一言系列
- **智谱AI**：ChatGLM系列
- **讯飞**：星火认知大模型
- **月之暗面**：Moonshot (Kimi)
- **腾讯**：混元大模型
- **DeepSeek**：DeepSeek系列
- **Ollama**：本地部署模型（Llama 3, Mistral, Qwen, CodeLlama等）
- **自定义模型**：任何兼容OpenAI API格式的模型服务（OneAPI、FastChat、vLLM等）

---

## 2. 技术栈

### 2.1 前端技术栈

- **框架**：React 18+
- **语言**：TypeScript
- **构建工具**：Vite
- **UI组件库**：Ant Design / Material-UI
- **状态管理**：Zustand / Redux Toolkit
- **路由**：React Router v6
- **HTTP客户端**：Axios
- **Markdown渲染**：react-markdown
- **代码高亮**：prismjs / highlight.js

### 2.2 后端技术栈

- **框架**：FastAPI (Python 3.10+)
- **异步支持**：asyncio, httpx
- **ORM**：SQLAlchemy 2.0
- **数据验证**：Pydantic
- **认证**：JWT (python-jose)
- **密码加密**：passlib (bcrypt)
- **API文档**：自动生成Swagger/OpenAPI文档
- **LLM集成**：
  - **LangChain**：langchain, langchain-openai, langchain-anthropic, langchain-community
  - **Ollama**：ollama-python（官方Python SDK）
  - **其他SDK**：openai, anthropic, google-generativeai

### 2.3 数据库

- **主数据库**：SQLite 3
- **ORM**：SQLAlchemy

### 2.4 部署

- **容器化**：Docker + Docker Compose
- **Web服务器**：Nginx (反向代理)
- **进程管理**：Uvicorn

---

## 3. 系统架构设计

### 3.1 整体架构

```
┌─────────────────────────────────────────────┐
│           Frontend (React SPA)              │
│  ┌─────────┐  ┌──────────┐  ┌────────────┐ │
│  │  登录   │  │  对话    │  │  设置     │ │
│  │  注册   │  │  管理    │  │  工作流   │ │
│  └─────────┘  └──────────┘  └────────────┘ │
└─────────────────┬───────────────────────────┘
                  │ HTTP/REST API
┌─────────────────▼───────────────────────────┐
│         Backend (FastAPI)                   │
│  ┌──────────┐  ┌───────────┐  ┌──────────┐ │
│  │  认证    │  │  对话     │  │  LLM     │ │
│  │  模块    │  │  管理     │  │  适配器  │ │
│  └──────────┘  └───────────┘  └──────────┘ │
│  ┌──────────────────────────────────────┐  │
│  │      Webhook & API for n8n           │  │
│  └──────────────────────────────────────┘  │
└─────┬──────────────────────────────┬────────┘
      │                              │
      │                    ┌─────────▼─────────┐
      │                    │   n8n Workflows   │
      │                    │  (自动化引擎)      │
      │                    │ ┌───────────────┐ │
      │                    │ │定时任务       │ │
      │                    │ │Webhook        │ │
      │                    │ │数据处理       │ │
      │                    │ │LLM编排        │ │
      │                    │ └───────────────┘ │
      │                    └───────────────────┘
      │
┌─────▼──────────────────────────────────────┐
│         Database (SQLite)                  │
│  users | conversations | messages          │
│  api_keys | workflows | settings           │
└─────────────────┬──────────────────────────┘
                  │
┌─────────────────▼──────────────────────────┐
│         External LLM APIs                  │
│  OpenAI | Claude | Gemini | Ollama        │
│  国内厂商 | 自定义模型                      │
└────────────────────────────────────────────┘
```

### 3.2 前端架构

```
src/
├── components/          # 可复用组件
│   ├── Chat/           # 对话相关组件
│   ├── Auth/           # 认证相关组件
│   └── Common/         # 通用组件
├── pages/              # 页面组件
│   ├── Login/
│   ├── Register/
│   ├── Chat/
│   └── Settings/
├── store/              # 状态管理
├── services/           # API服务
├── hooks/              # 自定义Hooks
├── types/              # TypeScript类型定义
└── utils/              # 工具函数
```

### 3.3 后端架构

```
backend/
├── app/
│   ├── api/                  # API路由
│   │   ├── auth.py          # 认证接口
│   │   ├── chat.py          # 对话接口
│   │   ├── llm.py           # LLM调用接口
│   │   ├── ollama.py        # Ollama专用接口
│   │   └── settings.py      # 设置接口
│   ├── core/                # 核心配置
│   │   ├── config.py        # 配置管理
│   │   ├── security.py      # 安全相关
│   │   └── database.py      # 数据库连接
│   ├── models/              # 数据模型
│   │   ├── user.py
│   │   ├── conversation.py
│   │   ├── message.py
│   │   └── api_key.py       # API密钥模型
│   ├── schemas/             # Pydantic模型
│   ├── services/            # 业务逻辑
│   │   ├── auth_service.py
│   │   ├── chat_service.py
│   │   ├── llm_service.py
│   │   └── langchain_service.py  # LangChain集成服务
│   ├── adapters/            # LLM适配器
│   │   ├── base.py          # 基类
│   │   ├── openai.py
│   │   ├── claude.py
│   │   ├── gemini.py
│   │   ├── qwen.py
│   │   ├── ernie.py
│   │   ├── glm.py
│   │   ├── spark.py
│   │   ├── moonshot.py
│   │   ├── hunyuan.py
│   │   ├── deepseek.py
│   │   ├── ollama.py        # Ollama适配器
│   │   ├── custom.py        # 自定义模型适配器
│   │   └── langchain.py     # LangChain适配器
│   └── utils/               # 工具函数
│       ├── encryption.py
│       └── logger.py
├── tests/                   # 测试
└── main.py                  # 应用入口
```

---

## 4. 功能模块详细设计

### 4.1 用户认证模块

#### 4.1.1 功能需求

- 用户注册（用户名、邮箱、密码）
- 用户登录（JWT Token）
- 密码加密存储（bcrypt）
- Token刷新机制
- 用户信息管理

#### 4.1.2 数据模型

```python
class User:
    id: int (PK)
    username: str (unique)
    email: str (unique)
    hashed_password: str
    is_active: bool
    is_superuser: bool
    created_at: datetime
    updated_at: datetime
```

#### 4.1.3 API接口

- `POST /api/auth/register` - 用户注册
- `POST /api/auth/login` - 用户登录
- `POST /api/auth/refresh` - 刷新Token
- `GET /api/auth/me` - 获取当前用户信息
- `PUT /api/auth/me` - 更新用户信息

### 4.2 LLM API集成模块

#### 4.2.1 统一适配器模式

```python
class BaseLLMAdapter(ABC):
    """LLM适配器基类"""

    def __init__(self, api_key: str = None, base_url: str = None,
                 extra_params: dict = None):
        self.api_key = api_key
        self.base_url = base_url
        self.extra_params = extra_params or {}

    @abstractmethod
    async def chat(self, messages: List[Message],
                   model: str, **kwargs) -> ChatResponse:
        """同步对话接口"""
        pass

    @abstractmethod
    async def stream_chat(self, messages: List[Message],
                          model: str, **kwargs) -> AsyncIterator:
        """流式对话接口"""
        pass

    @abstractmethod
    def get_models(self) -> List[str]:
        """获取可用模型列表"""
        pass

    @abstractmethod
    def validate_config(self) -> bool:
        """验证配置是否有效（连接测试）"""
        pass
```

#### 4.2.1.1 LangChain集成适配器

```python
class LangChainAdapter(BaseLLMAdapter):
    """基于LangChain的适配器，支持更高级的对话能力"""

    def __init__(self, provider: str, api_key: str = None,
                 base_url: str = None, **kwargs):
        super().__init__(api_key, base_url, kwargs)
        self.llm = self._initialize_langchain_llm(provider)
        self.memory = ConversationBufferMemory()

    def _initialize_langchain_llm(self, provider: str):
        """根据provider初始化对应的LangChain LLM"""
        if provider == "openai":
            return ChatOpenAI(api_key=self.api_key, base_url=self.base_url)
        elif provider == "anthropic":
            return ChatAnthropic(api_key=self.api_key)
        elif provider == "ollama":
            return ChatOllama(base_url=self.base_url or "http://localhost:11434")
        # ... 其他厂商

    async def chat_with_chain(self, messages: List[Message],
                             chain_type: str = "conversation") -> ChatResponse:
        """使用LangChain链进行对话"""
        pass
```

#### 4.2.1.2 Ollama适配器

```python
class OllamaAdapter(BaseLLMAdapter):
    """Ollama本地模型适配器"""

    def __init__(self, base_url: str = "http://localhost:11434"):
        super().__init__(base_url=base_url)
        self.client = ollama.AsyncClient(host=base_url)

    async def chat(self, messages: List[Message],
                   model: str, **kwargs) -> ChatResponse:
        """调用Ollama模型"""
        response = await self.client.chat(
            model=model,
            messages=[{"role": m.role, "content": m.content}
                     for m in messages],
            **kwargs
        )
        return ChatResponse(
            content=response['message']['content'],
            model=model,
            finish_reason=response.get('done_reason')
        )

    async def get_models(self) -> List[str]:
        """获取本地Ollama可用模型"""
        models = await self.client.list()
        return [m['name'] for m in models['models']]

    async def pull_model(self, model_name: str) -> AsyncIterator:
        """下载Ollama模型，返回下载进度"""
        async for progress in self.client.pull(model_name, stream=True):
            yield progress
```

#### 4.2.1.3 自定义模型适配器

```python
class CustomAdapter(BaseLLMAdapter):
    """自定义模型适配器，支持任何兼容OpenAI API格式的服务"""

    def __init__(self, api_key: str, base_url: str,
                 models: List[str] = None, **kwargs):
        super().__init__(api_key, base_url, kwargs)
        self.custom_models = models or []
        self.client = httpx.AsyncClient(
            base_url=base_url,
            headers={"Authorization": f"Bearer {api_key}"}
        )

    async def chat(self, messages: List[Message],
                   model: str, **kwargs) -> ChatResponse:
        """兼容OpenAI格式的API调用"""
        payload = {
            "model": model,
            "messages": [{"role": m.role, "content": m.content}
                        for m in messages],
            **self.extra_params,
            **kwargs
        }
        response = await self.client.post("/v1/chat/completions",
                                          json=payload)
        data = response.json()
        return ChatResponse(
            content=data['choices'][0]['message']['content'],
            model=model,
            tokens=data.get('usage', {})
        )

    def get_models(self) -> List[str]:
        """返回配置的自定义模型列表"""
        return self.custom_models
```

#### 4.2.2 厂商配置结构

**标准厂商配置**：

```json
{
  "provider": "openai",
  "name": "OpenAI",
  "api_key": "sk-xxx",
  "base_url": "https://api.openai.com/v1",
  "models": [
    {
      "id": "gpt-4",
      "name": "GPT-4",
      "max_tokens": 8192
    }
  ]
}
```

**Ollama配置**：

```json
{
  "provider": "ollama",
  "name": "Ollama本地模型",
  "base_url": "http://localhost:11434",
  "models": []  // 自动从Ollama服务获取
}
```

**自定义模型配置**：

```json
{
  "provider": "custom",
  "name": "我的自定义模型",
  "api_key": "custom-key-xxx",
  "base_url": "https://my-model-service.com",
  "models": [
    {
      "id": "my-model-v1",
      "name": "自定义模型 V1",
      "max_tokens": 4096
    }
  ],
  "extra_params": {
    "temperature": 0.7,
    "top_p": 0.9,
    "custom_header": "value"
  },
  "api_format": "openai"  // openai/claude/custom
}
```

**LangChain增强配置**：

```json
{
  "provider": "openai",
  "name": "OpenAI (LangChain)",
  "api_key": "sk-xxx",
  "use_langchain": true,
  "langchain_features": {
    "enable_memory": true,
    "enable_tools": false,
    "enable_agents": false
  }
}
```

#### 4.2.3 数据模型

```python
class APIKey:
    id: int (PK)
    user_id: int (FK)
    provider: str (openai/claude/gemini/ollama/custom/...)
    provider_name: str  # 自定义显示名称
    api_key: str (encrypted, nullable for Ollama)
    base_url: str (optional, 自定义API端点)
    models: JSON  # 自定义模型列表 [{"id": "model-1", "name": "Model 1"}]
    extra_params: JSON (optional)  # 额外参数 {"temperature": 0.7, ...}
    api_format: str (openai/claude/custom)  # API格式类型
    use_langchain: bool (default: False)  # 是否启用LangChain
    langchain_config: JSON (optional)  # LangChain配置
    is_active: bool
    created_at: datetime
    updated_at: datetime
```

#### 4.2.4 API接口

- `POST /api/llm/keys` - 添加API密钥（支持自定义配置）
- `GET /api/llm/keys` - 获取所有密钥（隐藏敏感信息）
- `PUT /api/llm/keys/{id}` - 更新密钥
- `DELETE /api/llm/keys/{id}` - 删除密钥
- `POST /api/llm/keys/{id}/test` - 测试连接（验证配置）
- `GET /api/llm/providers` - 获取所有支持的厂商
- `GET /api/llm/models/{provider}` - 获取厂商模型列表
- `GET /api/llm/ollama/models` - 获取本地Ollama可用模型
- `POST /api/llm/ollama/pull` - 下载Ollama模型（流式返回进度）
- `DELETE /api/llm/ollama/models/{model}` - 删除Ollama模型
- `GET /api/llm/custom/validate` - 验证自定义模型配置

### 4.3 对话管理模块

#### 4.3.1 功能需求

- 创建新对话
- 删除对话
- 重命名对话
- 对话列表展示
- 对话搜索
- 对话归档

#### 4.3.2 数据模型

```python
class Conversation:
    id: int (PK)
    user_id: int (FK)
    title: str
    provider: str
    model: str
    system_prompt: str (optional)
    is_archived: bool
    created_at: datetime
    updated_at: datetime

class Message:
    id: int (PK)
    conversation_id: int (FK)
    role: str (user/assistant/system)
    content: str
    tokens: int (optional)
    created_at: datetime
```

#### 4.3.3 API接口

- `POST /api/conversations` - 创建对话
- `GET /api/conversations` - 获取对话列表
- `GET /api/conversations/{id}` - 获取对话详情
- `PUT /api/conversations/{id}` - 更新对话
- `DELETE /api/conversations/{id}` - 删除对话
- `POST /api/conversations/{id}/messages` - 发送消息
- `GET /api/conversations/{id}/messages` - 获取消息列表
- `POST /api/conversations/{id}/stream` - 流式对话（SSE）

### 4.4 历史记录模块

#### 4.4.1 功能需求

- 完整的消息历史存储
- 按时间/对话检索
- 全文搜索
- 导出对话记录（JSON/Markdown）
- 统计信息（Token使用量、对话次数等）

#### 4.4.2 API接口

- `GET /api/history/search?q={query}` - 搜索历史
- `GET /api/history/stats` - 获取统计信息
- `GET /api/history/export/{conversation_id}` - 导出对话

---

## 5. 数据库设计

### 5.1 ER图

```
┌──────────────┐       ┌──────────────────┐       ┌──────────────┐
│    users     │──────<│  conversations   │>──────│   messages   │
└──────────────┘       └──────────────────┘       └──────────────┘
       │
       │
       ▼
┌──────────────┐
│   api_keys   │
└──────────────┘
```

### 5.2 表结构详细设计

#### users 表

```sql
CREATE TABLE users (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    username VARCHAR(50) UNIQUE NOT NULL,
    email VARCHAR(100) UNIQUE NOT NULL,
    hashed_password VARCHAR(255) NOT NULL,
    is_active BOOLEAN DEFAULT TRUE,
    is_superuser BOOLEAN DEFAULT FALSE,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
```

#### api_keys 表

```sql
CREATE TABLE api_keys (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    user_id INTEGER NOT NULL,
    provider VARCHAR(50) NOT NULL,
    provider_name VARCHAR(100),
    encrypted_api_key TEXT,  -- nullable for Ollama
    base_url VARCHAR(255),
    models TEXT,  -- JSON: [{"id": "model-1", "name": "Model 1", "max_tokens": 4096}]
    extra_params TEXT,  -- JSON: {"temperature": 0.7, "top_p": 0.9}
    api_format VARCHAR(20) DEFAULT 'openai',  -- openai/claude/custom
    use_langchain BOOLEAN DEFAULT FALSE,
    langchain_config TEXT,  -- JSON: {"enable_memory": true, ...}
    is_active BOOLEAN DEFAULT TRUE,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (user_id) REFERENCES users(id) ON DELETE CASCADE
);

CREATE INDEX idx_api_keys_user_id ON api_keys(user_id);
CREATE INDEX idx_api_keys_provider ON api_keys(provider);
```

#### conversations 表

```sql
CREATE TABLE conversations (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    user_id INTEGER NOT NULL,
    title VARCHAR(255) NOT NULL,
    provider VARCHAR(50) NOT NULL,
    model VARCHAR(100) NOT NULL,
    system_prompt TEXT,
    is_archived BOOLEAN DEFAULT FALSE,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (user_id) REFERENCES users(id) ON DELETE CASCADE
);

CREATE INDEX idx_conversations_user_id ON conversations(user_id);
CREATE INDEX idx_conversations_created_at ON conversations(created_at DESC);
```

#### messages 表

```sql
CREATE TABLE messages (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    conversation_id INTEGER NOT NULL,
    role VARCHAR(20) NOT NULL,
    content TEXT NOT NULL,
    tokens INTEGER,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (conversation_id) REFERENCES conversations(id) ON DELETE CASCADE
);

CREATE INDEX idx_messages_conversation_id ON messages(conversation_id);
CREATE INDEX idx_messages_created_at ON messages(created_at);
```

---

## 6. API接口设计

### 6.1 认证接口

#### POST /api/auth/register

**请求体**：

```json
{
  "username": "user123",
  "email": "user@example.com",
  "password": "securePassword123"
}
```

**响应**：

```json
{
  "id": 1,
  "username": "user123",
  "email": "user@example.com",
  "created_at": "2025-01-01T00:00:00Z"
}
```

#### POST /api/auth/login

**请求体**：

```json
{
  "username": "user123",
  "password": "securePassword123"
}
```

**响应**：

```json
{
  "access_token": "eyJhbGc...",
  "token_type": "bearer",
  "expires_in": 3600
}
```

### 6.2 对话接口

#### POST /api/conversations/{id}/messages

**请求体**：

```json
{
  "content": "你好，请介绍一下Python",
  "stream": true
}
```

**响应（非流式）**：

```json
{
  "id": 123,
  "role": "assistant",
  "content": "Python是一种...",
  "tokens": 150,
  "created_at": "2025-01-01T00:00:00Z"
}
```

**响应（流式SSE）**：

```
data: {"type": "start"}
data: {"type": "content", "delta": "Python"}
data: {"type": "content", "delta": "是"}
data: {"type": "end", "tokens": 150}
```

### 6.3 API密钥管理接口

#### POST /api/llm/keys

**请求体**：

```json
{
  "provider": "openai",
  "api_key": "sk-xxx",
  "base_url": "https://api.openai.com/v1"
}
```

---

## 7. 前端页面设计

### 7.1 页面结构

```
/                        # 重定向到 /chat
/login                   # 登录页
/register                # 注册页
/chat                    # 主对话页面
  ├── /chat/{id}        # 具体对话页面
/settings                # 设置页面
  ├── /settings/profile # 个人信息
  ├── /settings/api     # API密钥管理
  └── /settings/about   # 关于
```

### 7.2 主对话页面布局

```
┌─────────────────────────────────────────────────┐
│  Header (Logo, 用户信息, 设置)                  │
├──────────┬──────────────────────────────────────┤
│          │                                      │
│  侧边栏   │         对话区域                      │
│          │                                      │
│  - 新对话 │  ┌────────────────────────────────┐ │
│  - 今天   │  │  消息1 (用户)                  │ │
│  - 昨天   │  │  消息2 (AI)                   │ │
│  - 更早   │  │  ...                          │ │
│          │  └────────────────────────────────┘ │
│          │                                      │
│          │  ┌────────────────────────────────┐ │
│          │  │  输入框                        │ │
│          │  │  [模型选择] [发送]             │ │
│          │  └────────────────────────────────┘ │
└──────────┴──────────────────────────────────────┘
```

### 7.3 关键组件设计

#### ChatMessage组件

- Markdown渲染
- 代码高亮
- 复制功能
- 重新生成（针对AI消息）

#### ChatInput组件

- 多行输入
- Shift+Enter换行，Enter发送
- 模型选择下拉框
- 文件上传（预留）
- Token计数显示

#### ConversationList组件

- 按时间分组
- 搜索过滤
- 右键菜单（重命名、删除、归档）

---

## 8. 安全性考虑

### 8.1 认证安全

- 密码使用bcrypt加密，salt轮次：12
- JWT Token过期时间：1小时
- Refresh Token机制
- 防暴力破解：登录失败限制

### 8.2 API密钥安全

- 数据库存储使用AES-256加密
- 密钥不在日志中输出
- API响应时隐藏密钥，仅显示后4位
- 支持密钥权限管理

### 8.3 数据安全

- SQL注入防护（ORM参数化查询）
- XSS防护（前端内容转义）
- CORS配置
- Rate Limiting（频率限制）

### 8.4 隐私保护

- 数据完全本地存储
- 不向第三方发送用户数据
- 支持数据导出和删除

---

## 9. 开发计划

### Phase 1: 基础框架搭建（第1-2周）

**任务列表**：

- [ ] 项目初始化
  - [ ] 创建前端项目（React + Vite + TypeScript）
  - [ ] 创建后端项目（FastAPI + SQLAlchemy）
  - [ ] 配置ESLint、Prettier、Pre-commit hooks
- [ ] 数据库设计与实现
  - [ ] 创建SQLAlchemy模型
  - [ ] 编写数据库迁移脚本
  - [ ] 编写测试数据种子
- [ ] 基础配置
  - [ ] 环境变量管理
  - [ ] 日志系统配置
  - [ ] CORS配置

### Phase 2: 用户认证系统（第3周）

**任务列表**：

- [ ] 后端认证实现
  - [ ] JWT工具函数
  - [ ] 注册接口
  - [ ] 登录接口
  - [ ] 用户信息接口
- [ ] 前端认证页面
  - [ ] 登录页面UI
  - [ ] 注册页面UI
  - [ ] 认证状态管理
  - [ ] 路由守卫
- [ ] 测试
  - [ ] 单元测试
  - [ ] 集成测试

### Phase 3: LLM适配器实现（第4-5周）

**任务列表**：

- [ ] 适配器基类设计
  - [ ] 定义统一接口
  - [ ] 错误处理机制
  - [ ] 重试逻辑
- [ ] 实现主要厂商适配器
  - [ ] OpenAI适配器
  - [ ] Anthropic (Claude)适配器
  - [ ] Google Gemini适配器
  - [ ] 阿里通义千问适配器
  - [ ] 百度文心一言适配器
- [ ] API密钥管理
  - [ ] 密钥加密存储
  - [ ] 密钥CRUD接口
  - [ ] 前端密钥管理页面

### Phase 4: 对话功能实现（第6-7周）

**任务列表**：

- [ ] 后端对话接口
  - [ ] 创建对话
  - [ ] 发送消息（非流式）
  - [ ] 流式响应（SSE）
  - [ ] 消息历史
- [ ] 前端对话界面
  - [ ] 对话列表侧边栏
  - [ ] 消息展示组件
  - [ ] 消息输入组件
  - [ ] Markdown渲染
  - [ ] 代码高亮
- [ ] 实时通信
  - [ ] 流式消息接收
  - [ ] 打字动画效果

### Phase 5: 历史记录与高级功能（第8周）

**任务列表**：

- [ ] 历史记录功能
  - [ ] 全文搜索
  - [ ] 对话导出
  - [ ] 统计面板
- [ ] 高级功能
  - [ ] 对话归档
  - [ ] 对话分享（可选）
  - [ ] 系统提示词管理
  - [ ] 模型参数配置（温度、top_p等）

### Phase 6: 测试与优化（第9周）

**任务列表**：

- [ ] 测试
  - [ ] 单元测试覆盖
  - [ ] 集成测试
  - [ ] E2E测试
  - [ ] 性能测试
- [ ] 优化
  - [ ] 前端性能优化
  - [ ] 数据库查询优化
  - [ ] 错误处理完善
  - [ ] 日志完善

### Phase 7: Docker化与文档（第10周）

**任务列表**：

- [ ] Docker配置
  - [ ] Dockerfile编写
  - [ ] docker-compose.yml配置
  - [ ] 环境变量配置示例
- [ ] 文档编写
  - [ ] README.md
  - [ ] 部署文档
  - [ ] API文档
  - [ ] 用户手册

---

## 10. 项目目录结构

```
LLM/
├── frontend/                  # 前端项目
│   ├── public/
│   ├── src/
│   │   ├── assets/           # 静态资源
│   │   ├── components/       # 组件
│   │   │   ├── Auth/
│   │   │   │   ├── Login.tsx
│   │   │   │   └── Register.tsx
│   │   │   ├── Chat/
│   │   │   │   ├── ChatMessage.tsx
│   │   │   │   ├── ChatInput.tsx
│   │   │   │   ├── ConversationList.tsx
│   │   │   │   └── MessageList.tsx
│   │   │   └── Common/
│   │   │       ├── Header.tsx
│   │   │       ├── Sidebar.tsx
│   │   │       └── Loading.tsx
│   │   ├── pages/
│   │   │   ├── Login.tsx
│   │   │   ├── Register.tsx
│   │   │   ├── Chat.tsx
│   │   │   └── Settings/
│   │   │       ├── Profile.tsx
│   │   │       ├── APIKeys.tsx
│   │   │       └── About.tsx
│   │   ├── store/            # 状态管理
│   │   │   ├── authStore.ts
│   │   │   ├── chatStore.ts
│   │   │   └── settingsStore.ts
│   │   ├── services/         # API服务
│   │   │   ├── api.ts
│   │   │   ├── auth.ts
│   │   │   ├── chat.ts
│   │   │   └── llm.ts
│   │   ├── hooks/            # 自定义Hooks
│   │   │   ├── useAuth.ts
│   │   │   ├── useChat.ts
│   │   │   └── useStream.ts
│   │   ├── types/            # TypeScript类型
│   │   │   ├── auth.ts
│   │   │   ├── chat.ts
│   │   │   └── llm.ts
│   │   ├── utils/            # 工具函数
│   │   │   ├── request.ts
│   │   │   ├── storage.ts
│   │   │   └── format.ts
│   │   ├── App.tsx
│   │   └── main.tsx
│   ├── package.json
│   ├── tsconfig.json
│   ├── vite.config.ts
│   └── .env.example
│
├── backend/                   # 后端项目
│   ├── app/
│   │   ├── api/              # API路由
│   │   │   ├── __init__.py
│   │   │   ├── auth.py
│   │   │   ├── chat.py
│   │   │   ├── llm.py
│   │   │   ├── ollama.py        # Ollama专用接口
│   │   │   └── settings.py
│   │   ├── core/             # 核心配置
│   │   │   ├── __init__.py
│   │   │   ├── config.py
│   │   │   ├── security.py
│   │   │   └── database.py
│   │   ├── models/           # 数据库模型
│   │   │   ├── __init__.py
│   │   │   ├── user.py
│   │   │   ├── conversation.py
│   │   │   ├── message.py
│   │   │   └── api_key.py
│   │   ├── schemas/          # Pydantic模型
│   │   │   ├── __init__.py
│   │   │   ├── user.py
│   │   │   ├── conversation.py
│   │   │   ├── message.py
│   │   │   └── llm.py
│   │   ├── services/         # 业务逻辑
│   │   │   ├── __init__.py
│   │   │   ├── auth_service.py
│   │   │   ├── chat_service.py
│   │   │   ├── llm_service.py
│   │   │   └── langchain_service.py  # LangChain集成服务
│   │   ├── adapters/         # LLM适配器
│   │   │   ├── __init__.py
│   │   │   ├── base.py
│   │   │   ├── openai.py
│   │   │   ├── claude.py
│   │   │   ├── gemini.py
│   │   │   ├── qwen.py
│   │   │   ├── ernie.py
│   │   │   ├── glm.py
│   │   │   ├── spark.py
│   │   │   ├── moonshot.py
│   │   │   ├── hunyuan.py
│   │   │   ├── deepseek.py
│   │   │   ├── ollama.py        # Ollama适配器
│   │   │   ├── custom.py        # 自定义模型适配器
│   │   │   └── langchain.py     # LangChain集成适配器
│   │   └── utils/            # 工具函数
│   │       ├── __init__.py
│   │       ├── encryption.py
│   │       └── logger.py
│   ├── tests/                # 测试
│   │   ├── test_auth.py
│   │   ├── test_chat.py
│   │   └── test_llm.py
│   ├── main.py               # 应用入口
│   ├── requirements.txt
│   ├── .env.example
│   └── pytest.ini
│
├── docker/                    # Docker配置
│   ├── Dockerfile.frontend
│   ├── Dockerfile.backend
│   └── nginx.conf
│
├── docker-compose.yml
├── .gitignore
├── README.md
└── 开发文档.md               # 本文档
```

---

## 11. 环境变量配置

### 11.1 后端环境变量 (.env)

```env
# 应用配置
APP_NAME=LLM Manager
APP_VERSION=1.0.0
DEBUG=False

# 数据库
DATABASE_URL=sqlite:///./data/llm.db

# JWT配置
SECRET_KEY=your-secret-key-change-this-in-production
ALGORITHM=HS256
ACCESS_TOKEN_EXPIRE_MINUTES=60

# 加密密钥（用于加密API密钥）
ENCRYPTION_KEY=your-encryption-key-32-bytes

# CORS
ALLOWED_ORIGINS=http://localhost:5173,http://localhost:3000

# 日志
LOG_LEVEL=INFO
```

### 11.2 前端环境变量 (.env)

```env
VITE_API_BASE_URL=http://localhost:8000
VITE_APP_NAME=LLM Manager
```

---

## 12. 部署方案

### 12.1 Docker Compose部署（推荐）

#### docker-compose.yml示例

```yaml
version: '3.8'

services:
  backend:
    build:
      context: ./backend
      dockerfile: ../docker/Dockerfile.backend
    container_name: llm-backend
    ports:
      - "8000:8000"
    volumes:
      - ./data:/app/data
      - ./backend/.env:/app/.env
    environment:
      - DATABASE_URL=sqlite:///./data/llm.db
    restart: unless-stopped

  frontend:
    build:
      context: ./frontend
      dockerfile: ../docker/Dockerfile.frontend
    container_name: llm-frontend
    ports:
      - "80:80"
    depends_on:
      - backend
    restart: unless-stopped

volumes:
  data:
```

### 12.2 启动命令

```bash
# 构建镜像
docker-compose build

# 启动服务
docker-compose up -d

# 查看日志
docker-compose logs -f

# 停止服务
docker-compose down
```

### 12.3 本地开发启动

#### 后端

```bash
cd backend
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate
pip install -r requirements.txt
uvicorn main:app --reload --host 0.0.0.0 --port 8000
```

#### 前端

```bash
cd frontend
npm install
npm run dev
```

---

## 13. 测试策略

### 13.1 后端测试

```bash
# 运行所有测试
pytest

# 运行带覆盖率的测试
pytest --cov=app --cov-report=html

# 运行特定测试
pytest tests/test_auth.py
```

### 13.2 前端测试

```bash
# 单元测试
npm run test

# E2E测试
npm run test:e2e
```

---

## 14. 扩展功能（后续版本）

### 14.1 Phase 2功能规划

- [ ] 多模态支持（图片、文件上传）
- [ ] 对话分享功能
- [ ] 团队协作功能
- [ ] 自定义插件系统
- [ ] 语音输入/输出
- [ ] 多语言支持

### 14.2 性能优化

- [ ] 消息分页加载
- [ ] 虚拟滚动
- [ ] Redis缓存层
- [ ] CDN静态资源

---

## 15. 注意事项

### 15.1 开发规范

- 遵循PEP 8（Python）和Airbnb Style Guide（TypeScript）
- 所有API接口必须编写单元测试
- Git提交信息遵循Conventional Commits规范
- 代码审查必须通过后才能合并

### 15.2 安全规范

- 永远不要将API密钥、Secret Key提交到Git
- 定期更新依赖包，修复安全漏洞
- 生产环境必须使用HTTPS
- 定期备份数据库

### 15.3 性能要求

- API响应时间 < 200ms（不含LLM调用）
- 首屏加载时间 < 2s
- 流式响应延迟 < 100ms
- 支持至少100个并发用户

---

## 16. Ollama、LangChain 和自定义模型集成指南

### 16.1 Ollama 集成

#### 16.1.1 Ollama 简介

Ollama 是一个轻量级的本地大模型运行框架，支持 Llama 3、Mistral、Qwen、CodeLlama 等开源模型。

#### 16.1.2 Ollama 安装

```bash
# macOS/Linux
curl -fsSL https://ollama.com/install.sh | sh

# Windows
# 下载并安装 Ollama for Windows
# https://ollama.com/download/windows
```

#### 16.1.3 使用 Ollama

```bash
# 拉取模型
ollama pull llama3
ollama pull mistral
ollama pull qwen:7b

# 查看已安装的模型
ollama list

# 运行 Ollama 服务（默认端口 11434）
ollama serve
```

#### 16.1.4 在应用中配置 Ollama

在设置页面添加 Ollama 配置：

```json
{
  "provider": "ollama",
  "provider_name": "Ollama 本地模型",
  "base_url": "http://localhost:11434",
  "models": []  // 自动从 Ollama 获取
}
```

#### 16.1.5 Ollama 适配器示例

```python
# backend/app/adapters/ollama.py
import ollama
from typing import List, AsyncIterator
from .base import BaseLLMAdapter

class OllamaAdapter(BaseLLMAdapter):
    def __init__(self, base_url: str = "http://localhost:11434"):
        super().__init__(base_url=base_url)
        self.client = ollama.AsyncClient(host=base_url)

    async def chat(self, messages, model, **kwargs):
        response = await self.client.chat(
            model=model,
            messages=[{"role": m.role, "content": m.content}
                     for m in messages],
            stream=False,
            **kwargs
        )
        return response['message']['content']

    async def stream_chat(self, messages, model, **kwargs):
        async for chunk in await self.client.chat(
            model=model,
            messages=[{"role": m.role, "content": m.content}
                     for m in messages],
            stream=True,
            **kwargs
        ):
            if 'message' in chunk:
                yield chunk['message']['content']

    async def get_models(self):
        models = await self.client.list()
        return [m['name'] for m in models['models']]

    async def pull_model(self, model_name: str):
        """下载模型，返回进度"""
        async for progress in self.client.pull(model_name, stream=True):
            yield {
                "status": progress.get('status'),
                "completed": progress.get('completed', 0),
                "total": progress.get('total', 0)
            }
```

---

### 16.2 LangChain 集成

#### 16.2.1 LangChain 简介

LangChain 是一个强大的 LLM 应用开发框架，提供：

- 统一的 LLM 接口
- 对话记忆管理
- 工具调用（Function Calling）
- Agent 能力
- 向量存储和检索

#### 16.2.2 安装 LangChain

```bash
pip install langchain
pip install langchain-openai
pip install langchain-anthropic
pip install langchain-community
pip install langchain-ollama
```

#### 16.2.3 LangChain 基础使用

```python
# backend/app/services/langchain_service.py
from langchain_openai import ChatOpenAI
from langchain_anthropic import ChatAnthropic
from langchain_community.chat_models import ChatOllama
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationChain

class LangChainService:
    def __init__(self):
        self.llm_cache = {}
        self.memory_cache = {}

    def get_llm(self, provider: str, api_key: str = None,
                base_url: str = None, model: str = None):
        """获取 LangChain LLM 实例"""
        cache_key = f"{provider}:{model}"
        if cache_key in self.llm_cache:
            return self.llm_cache[cache_key]

        if provider == "openai":
            llm = ChatOpenAI(
                api_key=api_key,
                base_url=base_url,
                model=model or "gpt-3.5-turbo"
            )
        elif provider == "anthropic":
            llm = ChatAnthropic(
                api_key=api_key,
                model=model or "claude-3-5-sonnet-20241022"
            )
        elif provider == "ollama":
            llm = ChatOllama(
                base_url=base_url or "http://localhost:11434",
                model=model or "llama3"
            )
        else:
            raise ValueError(f"Unsupported provider: {provider}")

        self.llm_cache[cache_key] = llm
        return llm

    def create_conversation_chain(self, llm, conversation_id: str):
        """创建带记忆的对话链"""
        if conversation_id not in self.memory_cache:
            self.memory_cache[conversation_id] = ConversationBufferMemory()

        chain = ConversationChain(
            llm=llm,
            memory=self.memory_cache[conversation_id],
            verbose=True
        )
        return chain

    async def chat_with_memory(self, provider: str, model: str,
                               conversation_id: str, message: str,
                               api_key: str = None, base_url: str = None):
        """带记忆的对话"""
        llm = self.get_llm(provider, api_key, base_url, model)
        chain = self.create_conversation_chain(llm, conversation_id)
        response = await chain.arun(message)
        return response
```

#### 16.2.4 在 API 中使用 LangChain

```python
# backend/app/api/chat.py
from fastapi import APIRouter, Depends
from app.services.langchain_service import LangChainService

router = APIRouter()
langchain_service = LangChainService()

@router.post("/conversations/{conversation_id}/chat/langchain")
async def chat_with_langchain(
    conversation_id: int,
    message: str,
    provider: str,
    model: str,
    api_key: str = None,
    base_url: str = None
):
    """使用 LangChain 进行对话"""
    response = await langchain_service.chat_with_memory(
        provider=provider,
        model=model,
        conversation_id=str(conversation_id),
        message=message,
        api_key=api_key,
        base_url=base_url
    )
    return {"response": response}
```

#### 16.2.5 LangChain 高级功能

**工具调用示例**：

```python
from langchain.agents import create_openai_functions_agent, AgentExecutor
from langchain.tools import tool

@tool
def get_weather(city: str) -> str:
    """获取指定城市的天气信息"""
    # 实际应该调用天气 API
    return f"{city}今天天气晴朗，气温25度"

def create_agent_with_tools(llm):
    tools = [get_weather]
    agent = create_openai_functions_agent(llm, tools)
    agent_executor = AgentExecutor(agent=agent, tools=tools)
    return agent_executor
```

---

### 16.3 自定义模型集成

#### 16.3.1 支持的自定义模型服务

- **OneAPI**：多模型聚合网关
- **FastChat**：基于 FastChat 的模型服务
- **vLLM**：高性能推理引擎
- **Text Generation Inference**：HuggingFace 推理服务
- **LocalAI**：本地 AI API 服务
- 任何兼容 OpenAI API 格式的服务

#### 16.3.2 配置自定义模型

在设置页面添加自定义模型：

```json
{
  "provider": "custom",
  "provider_name": "我的自定义模型服务",
  "api_key": "your-api-key",
  "base_url": "https://your-model-service.com/v1",
  "models": [
    {
      "id": "my-llama-70b",
      "name": "自定义 Llama 70B",
      "max_tokens": 8192
    },
    {
      "id": "my-mixtral-8x7b",
      "name": "自定义 Mixtral 8x7B",
      "max_tokens": 32768
    }
  ],
  "extra_params": {
    "temperature": 0.7,
    "top_p": 0.9
  },
  "api_format": "openai"
}
```

#### 16.3.3 自定义模型适配器

```python
# backend/app/adapters/custom.py
import httpx
from typing import List, AsyncIterator
from .base import BaseLLMAdapter

class CustomAdapter(BaseLLMAdapter):
    def __init__(self, api_key: str, base_url: str,
                 models: List[dict] = None, **kwargs):
        super().__init__(api_key, base_url, kwargs)
        self.custom_models = models or []
        self.client = httpx.AsyncClient(
            base_url=base_url,
            headers={"Authorization": f"Bearer {api_key}"},
            timeout=60.0
        )

    async def chat(self, messages, model, **kwargs):
        """兼容 OpenAI 格式的 API 调用"""
        payload = {
            "model": model,
            "messages": [{"role": m.role, "content": m.content}
                        for m in messages],
            **self.extra_params,
            **kwargs
        }

        response = await self.client.post(
            "/chat/completions",
            json=payload
        )
        response.raise_for_status()
        data = response.json()

        return {
            "content": data['choices'][0]['message']['content'],
            "model": model,
            "usage": data.get('usage', {})
        }

    async def stream_chat(self, messages, model, **kwargs):
        """流式对话"""
        payload = {
            "model": model,
            "messages": [{"role": m.role, "content": m.content}
                        for m in messages],
            "stream": True,
            **self.extra_params,
            **kwargs
        }

        async with self.client.stream(
            "POST",
            "/chat/completions",
            json=payload
        ) as response:
            async for line in response.aiter_lines():
                if line.startswith("data: "):
                    data = line[6:]
                    if data == "[DONE]":
                        break
                    import json
                    chunk = json.loads(data)
                    if 'choices' in chunk and len(chunk['choices']) > 0:
                        delta = chunk['choices'][0].get('delta', {})
                        if 'content' in delta:
                            yield delta['content']

    def get_models(self):
        return [m['id'] for m in self.custom_models]

    async def validate_config(self):
        """验证配置是否有效"""
        try:
            response = await self.client.get("/models")
            return response.status_code == 200
        except:
            return False
```

#### 16.3.4 前端配置界面

在设置页面提供自定义模型配置表单：

- Provider 类型选择（Custom）
- 自定义显示名称
- API Base URL
- API Key
- 模型列表配置（支持添加多个模型）
- 额外参数配置（JSON 格式）
- 连接测试按钮

#### 16.3.5 OneAPI 集成示例

OneAPI 是一个流行的多模型聚合网关，配置示例：

```json
{
  "provider": "custom",
  "provider_name": "OneAPI 网关",
  "api_key": "sk-oneapi-xxx",
  "base_url": "https://your-oneapi-domain.com/v1",
  "models": [
    {"id": "gpt-4", "name": "GPT-4 (OneAPI)"},
    {"id": "claude-3", "name": "Claude 3 (OneAPI)"},
    {"id": "gemini-pro", "name": "Gemini Pro (OneAPI)"}
  ],
  "api_format": "openai"
}
```

---

### 16.4 模型配置最佳实践

#### 16.4.1 Ollama 最佳实践

- 根据硬件配置选择合适的模型大小（7B/13B/70B）
- 使用量化模型（如 Q4_K_M）以节省内存
- 本地开发时使用 Ollama，生产环境考虑云端服务
- 定期更新模型到最新版本

#### 16.4.2 LangChain 最佳实践

- 使用 ConversationBufferMemory 管理对话历史
- 生产环境使用 Redis 存储对话状态
- 合理设置 Token 限制，避免超出上下文长度
- 使用缓存减少 API 调用次数

#### 16.4.3 自定义模型最佳实践

- 始终提供连接测试功能
- 保存配置前验证 API 可用性
- 提供详细的错误信息和日志
- 支持自定义 HTTP 头和参数
- 实现重试机制和超时控制

#### 16.4.4 安全建议

- API Key 必须加密存储
- 使用 HTTPS 连接自定义服务
- 实施 Rate Limiting 防止滥用
- 记录所有 API 调用日志
- 定期轮换 API 密钥

---

## 16.5 n8n 工作流自动化集成

### 16.5.1 n8n 简介

n8n 是一个开源的工作流自动化平台，可以连接 200+ 种服务，实现复杂的自动化场景。集成 n8n 后，你的 LLM 平台可以实现：

- **定时任务**：定期执行 LLM 任务（如每日摘要、报告生成）
- **Webhook 触发**：通过外部事件触发 LLM 对话
- **多步骤工作流**：组合多个 LLM 调用和数据处理
- **外部集成**：连接 Slack、Email、数据库等服务
- **数据管道**：批量处理数据并使用 LLM 分析

### 16.5.2 n8n 安装和配置

#### Docker Compose 配置

在 `docker-compose.yml` 中添加 n8n 服务：

```yaml
version: '3.8'

services:
  backend:
    # ... 现有配置

  frontend:
    # ... 现有配置

  n8n:
    image: n8nio/n8n:latest
    container_name: llm-n8n
    ports:
      - "5678:5678"
    environment:
      - N8N_BASIC_AUTH_ACTIVE=true
      - N8N_BASIC_AUTH_USER=admin
      - N8N_BASIC_AUTH_PASSWORD=changeme
      - N8N_HOST=0.0.0.0
      - N8N_PORT=5678
      - N8N_PROTOCOL=http
      - WEBHOOK_URL=http://localhost:5678/
      - GENERIC_TIMEZONE=Asia/Shanghai
    volumes:
      - n8n_data:/home/node/.n8n
    networks:
      - llm-network
    restart: unless-stopped

volumes:
  n8n_data:

networks:
  llm-network:
    driver: bridge
```

#### 启动 n8n

```bash
docker-compose up -d n8n

# 访问 n8n Web UI
# http://localhost:5678
```

### 16.5.3 为 n8n 提供 API 接口

在后端添加专门的 n8n API 接口：

```python
# backend/app/api/n8n.py
from fastapi import APIRouter, Depends, HTTPException
from pydantic import BaseModel
from typing import List, Optional
from app.services.llm_service import LLMService

router = APIRouter(prefix="/api/n8n", tags=["n8n"])

class N8nChatRequest(BaseModel):
    provider: str
    model: str
    messages: List[dict]
    api_key: Optional[str] = None
    temperature: Optional[float] = 0.7
    max_tokens: Optional[int] = None

class N8nChatResponse(BaseModel):
    content: str
    usage: dict
    model: str

@router.post("/chat", response_model=N8nChatResponse)
async def n8n_chat(request: N8nChatRequest):
    """
    n8n 专用的 LLM 调用接口
    无需认证，通过 API Key 直接调用
    """
    try:
        llm_service = LLMService()
        response = await llm_service.chat(
            provider=request.provider,
            model=request.model,
            messages=request.messages,
            api_key=request.api_key,
            temperature=request.temperature,
            max_tokens=request.max_tokens
        )
        return N8nChatResponse(
            content=response['content'],
            usage=response.get('usage', {}),
            model=request.model
        )
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@router.post("/batch-chat")
async def n8n_batch_chat(requests: List[N8nChatRequest]):
    """批量 LLM 调用，用于 n8n 批处理场景"""
    results = []
    for req in requests:
        try:
            llm_service = LLMService()
            response = await llm_service.chat(
                provider=req.provider,
                model=req.model,
                messages=req.messages,
                api_key=req.api_key,
                temperature=req.temperature,
                max_tokens=req.max_tokens
            )
            results.append({
                "success": True,
                "content": response['content'],
                "usage": response.get('usage', {})
            })
        except Exception as e:
            results.append({
                "success": False,
                "error": str(e)
            })
    return {"results": results}

@router.get("/providers")
async def get_providers():
    """获取所有可用的 LLM 厂商"""
    return {
        "providers": [
            {"id": "openai", "name": "OpenAI"},
            {"id": "claude", "name": "Anthropic Claude"},
            {"id": "gemini", "name": "Google Gemini"},
            {"id": "ollama", "name": "Ollama"},
            # ... 其他厂商
        ]
    }

@router.post("/webhook/{conversation_id}")
async def webhook_message(conversation_id: int, message: str):
    """
    Webhook 接口，用于从 n8n 接收消息
    自动创建对话并返回 LLM 响应
    """
    # 实现逻辑
    pass
```

### 16.5.4 n8n 工作流示例

#### 示例 1：定时生成每日报告

```json
{
  "name": "每日 AI 摘要",
  "nodes": [
    {
      "name": "Cron 触发器",
      "type": "n8n-nodes-base.cron",
      "parameters": {
        "triggerTimes": {
          "item": [
            {
              "hour": 9,
              "minute": 0
            }
          ]
        }
      }
    },
    {
      "name": "获取数据",
      "type": "n8n-nodes-base.httpRequest",
      "parameters": {
        "url": "https://api.example.com/daily-data",
        "method": "GET"
      }
    },
    {
      "name": "调用 LLM 生成摘要",
      "type": "n8n-nodes-base.httpRequest",
      "parameters": {
        "url": "http://backend:8000/api/n8n/chat",
        "method": "POST",
        "bodyParameters": {
          "parameters": [
            {
              "name": "provider",
              "value": "openai"
            },
            {
              "name": "model",
              "value": "gpt-4"
            },
            {
              "name": "messages",
              "value": "={{[$json.data]}}"
            }
          ]
        }
      }
    },
    {
      "name": "发送邮件",
      "type": "n8n-nodes-base.emailSend",
      "parameters": {
        "toEmail": "admin@example.com",
        "subject": "每日 AI 摘要",
        "text": "={{$json.content}}"
      }
    }
  ]
}
```

#### 示例 2：Webhook 触发对话

在 n8n 中创建 Webhook 节点：

1. 添加 Webhook 触发器
2. 解析请求数据
3. 调用 LLM API
4. 返回响应

```javascript
// n8n Function 节点示例
const message = $input.item.json.message;
const provider = $input.item.json.provider || 'openai';
const model = $input.item.json.model || 'gpt-3.5-turbo';

return {
  json: {
    provider: provider,
    model: model,
    messages: [
      {
        role: 'user',
        content: message
      }
    ],
    temperature: 0.7
  }
};
```

#### 示例 3：多模型对比

```json
{
  "name": "多模型对比分析",
  "nodes": [
    {
      "name": "手动触发",
      "type": "n8n-nodes-base.manualTrigger"
    },
    {
      "name": "准备提示词",
      "type": "n8n-nodes-base.set",
      "parameters": {
        "values": {
          "string": [
            {
              "name": "prompt",
              "value": "解释量子计算的基本原理"
            }
          ]
        }
      }
    },
    {
      "name": "并行调用多个模型",
      "type": "n8n-nodes-base.splitInBatches"
    },
    {
      "name": "GPT-4",
      "type": "n8n-nodes-base.httpRequest",
      "parameters": {
        "url": "http://backend:8000/api/n8n/chat",
        "method": "POST",
        "body": {
          "provider": "openai",
          "model": "gpt-4",
          "messages": [{"role": "user", "content": "={{$json.prompt}}"}]
        }
      }
    },
    {
      "name": "Claude",
      "type": "n8n-nodes-base.httpRequest",
      "parameters": {
        "url": "http://backend:8000/api/n8n/chat",
        "method": "POST",
        "body": {
          "provider": "claude",
          "model": "claude-3-opus",
          "messages": [{"role": "user", "content": "={{$json.prompt}}"}]
        }
      }
    },
    {
      "name": "合并结果",
      "type": "n8n-nodes-base.merge"
    }
  ]
}
```

### 16.5.5 在前端集成 n8n

#### 添加工作流管理页面

```typescript
// frontend/src/pages/Workflows/index.tsx
import React, { useState, useEffect } from 'react';
import { Card, Button, List, Tag } from 'antd';

const WorkflowsPage: React.FC = () => {
  const [workflows, setWorkflows] = useState([]);

  const openN8n = () => {
    // 在新标签页打开 n8n
    window.open('http://localhost:5678', '_blank');
  };

  return (
    <div>
      <Card
        title="工作流管理"
        extra={
          <Button type="primary" onClick={openN8n}>
            打开 n8n 编辑器
          </Button>
        }
      >
        <p>使用 n8n 创建自动化工作流，实现：</p>
        <ul>
          <li>定时任务（每日摘要、报告生成）</li>
          <li>Webhook 触发（外部事件响应）</li>
          <li>批量处理（数据分析、内容生成）</li>
          <li>多步骤编排（复杂 LLM 工作流）</li>
        </ul>
      </Card>
    </div>
  );
};

export default WorkflowsPage;
```

### 16.5.6 n8n 工作流模板库

提供预置的工作流模板：

#### 模板 1：智能客服自动回复

- Webhook 接收用户消息
- 调用 LLM 生成回复
- 返回响应到客服系统

#### 模板 2：文档批量总结

- 定时读取文档目录
- 批量调用 LLM 生成摘要
- 保存到数据库或发送通知

#### 模板 3：多语言翻译管道

- 接收源文本
- 并行调用多个翻译模型
- 对比结果并选择最佳翻译

#### 模板 4：内容审核工作流

- 接收用户提交的内容
- 调用 LLM 进行内容审核
- 根据结果自动通过/拒绝

### 16.5.7 n8n 与 LangChain 结合

```python
# 在 n8n 中使用 LangChain 的 Agent
from langchain.agents import create_openai_functions_agent
from langchain.tools import tool

@tool
def search_database(query: str) -> str:
    """搜索数据库"""
    # 实现数据库搜索逻辑
    return "搜索结果..."

def n8n_langchain_agent(user_input: str):
    tools = [search_database]
    llm = ChatOpenAI(model="gpt-4")
    agent = create_openai_functions_agent(llm, tools)
    result = agent.invoke({"input": user_input})
    return result
```

### 16.5.8 安全配置

#### API 认证

```python
# 为 n8n API 添加认证
from fastapi import Header, HTTPException

async def verify_n8n_token(x_n8n_token: str = Header(...)):
    """验证 n8n 请求的 Token"""
    if x_n8n_token != os.getenv("N8N_API_TOKEN"):
        raise HTTPException(status_code=401, detail="Invalid token")
    return x_n8n_token

@router.post("/chat", dependencies=[Depends(verify_n8n_token)])
async def n8n_chat(request: N8nChatRequest):
    # ... 实现
    pass
```

#### 环境变量配置

```env
# backend/.env
N8N_API_TOKEN=your-secure-token-here
N8N_WEBHOOK_URL=http://n8n:5678/webhook
N8N_ENABLED=true
```

### 16.5.9 n8n 最佳实践

#### 性能优化

- 使用批处理减少 HTTP 请求次数
- 合理设置工作流超时时间
- 使用队列处理大量任务

#### 错误处理

- 添加错误捕获节点
- 配置失败重试机制
- 记录详细的执行日志

#### 监控和告警

- 使用 n8n 的执行历史功能
- 配置失败通知（Email/Slack）
- 定期检查工作流健康状态

#### 工作流组织

- 使用标签分类工作流
- 添加详细的描述文档
- 定期备份工作流配置

---

## 17. 参考资源

### 17.1 官方文档

- FastAPI: <https://fastapi.tiangolo.com/>
- React: <https://react.dev/>
- SQLAlchemy: <https://www.sqlalchemy.org/>
- Ant Design: <https://ant.design/>
- LangChain: <https://python.langchain.com/>
- Ollama: <https://ollama.com/>
- n8n: <https://docs.n8n.io/>

### 17.2 LLM厂商文档

- OpenAI API: <https://platform.openai.com/docs>
- Anthropic API: <https://docs.anthropic.com/>
- Google Gemini API: <https://ai.google.dev/>
- 阿里通义千问: <https://help.aliyun.com/zh/dashscope/>
- 百度文心一言: <https://cloud.baidu.com/doc/WENXINWORKSHOP/>
- 智谱AI: <https://open.bigmodel.cn/dev/api>
- 讯飞星火: <https://www.xfyun.cn/doc/spark/>
- Moonshot: <https://platform.moonshot.cn/docs>

---

## 附录A: 快速开始检查清单

开始开发前，请确保完成以下准备工作：

- [ ] 安装Python 3.10+
- [ ] 安装Node.js 18+
- [ ] 安装Docker和Docker Compose
- [ ] 安装Git
- [ ] 准备至少一个LLM厂商的API密钥
- [ ] 阅读完本文档
- [ ] 克隆项目仓库
- [ ] 配置开发环境

---

**文档版本**: v1.0
**最后更新**: 2025-12-15
**维护者**: 开发团队
